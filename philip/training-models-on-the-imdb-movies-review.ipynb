{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re, string, unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Ridge\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.linalg import inv\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Authenticate with Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Download the dataset\n",
    "dataset = 'lakshmi25npathi/imdb-dataset-of-50k-movie-reviews'\n",
    "path = '.'\n",
    "api.dataset_download_files(dataset, path=path, unzip=True)\n",
    "\n",
    "print(\"Dataset downloaded and extracted to:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the IMDB dataset directly from Kaggle's dataset path\n",
    "imdb_data = pd.read_csv('IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summary of the dataset\n",
    "imdb_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sentiment count\n",
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split the dataset  \n",
    "#train dataset\n",
    "train_reviews=imdb_data.review[:40000]\n",
    "train_sentiments=imdb_data.sentiment[:40000]\n",
    "#test dataset\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized train reviews\n",
    "norm_train_reviews=imdb_data.review[:40000]\n",
    "norm_train_reviews[0]\n",
    "#convert dataframe to string\n",
    "#norm_train_string=norm_train_reviews.to_string()\n",
    "#Spelling correction using Textblob\n",
    "#norm_train_spelling=TextBlob(norm_train_string)\n",
    "#norm_train_spelling.correct()\n",
    "#Tokenization using Textblob\n",
    "#norm_train_words=norm_train_spelling.words\n",
    "#norm_train_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized test reviews\n",
    "norm_test_reviews=imdb_data.review[40000:]\n",
    "norm_test_reviews[45005]\n",
    "##convert dataframe to string\n",
    "#norm_test_string=norm_test_reviews.to_string()\n",
    "#spelling correction using Textblob\n",
    "#norm_test_spelling=TextBlob(norm_test_string)\n",
    "#print(norm_test_spelling.correct())\n",
    "#Tokenization using Textblob\n",
    "#norm_test_words=norm_test_spelling.words\n",
    "#norm_test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=1,max_df=1.0,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)\n",
    "#vocab=cv.get_feature_names()-toget feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(min_df=1,max_df=1.0,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test_reviews)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeling the sentient data\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
    "print(sentiment_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the sentiment data\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:]\n",
    "print(train_sentiments)\n",
    "print(test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RVFL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model training and evaluation code (assuming preprocessed data is already available)\n",
    "\n",
    "# RVFL model definition\n",
    "class RVFL:\n",
    "    def __init__(self, n_nodes=100, random_weight_range=(-1, 1), random_bias_range=(-1, 1), regularization=0.01):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.random_weight_range = random_weight_range\n",
    "        self.random_bias_range = random_bias_range\n",
    "        self.regularization = regularization\n",
    "        self.input_weights = None\n",
    "        self.biases = None\n",
    "        self.output_weights = None\n",
    "        self.scaler = StandardScaler(with_mean=False)\n",
    "    \n",
    "    def _activation(self, X):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        self.input_weights = np.random.uniform(self.random_weight_range[0], self.random_weight_range[1], (X_train.shape[1], self.n_nodes))\n",
    "        self.biases = np.random.uniform(self.random_bias_range[0], self.random_bias_range[1], self.n_nodes)\n",
    "        H = self._activation(np.dot(X_train, self.input_weights) + self.biases)\n",
    "        H_augmented = np.hstack((X_train, H))\n",
    "        ridge_regressor = Ridge(alpha=self.regularization, fit_intercept=False)\n",
    "        ridge_regressor.fit(H_augmented, y_train)\n",
    "        self.output_weights = ridge_regressor.coef_\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        H = self._activation(np.dot(X_test, self.input_weights) + self.biases)\n",
    "        H_augmented = np.hstack((X_test, H))\n",
    "        y_pred = np.dot(H_augmented, self.output_weights)\n",
    "        return (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Assuming preprocessed and vectorized training and test data are available as:\n",
    "# cv_train_reviews, train_sentiments, cv_test_reviews, test_sentiments\n",
    "\n",
    "# Train and evaluate the RVFL model\n",
    "rvfl_model = RVFL(n_nodes=100, random_weight_range=(-1, 1), random_bias_range=(-1, 1), regularization=0.01)\n",
    "rvfl_model.fit(cv_train_reviews, train_sentiments)\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "y_pred = rvfl_model.predict(cv_test_reviews)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(test_sentiments, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "auc = roc_auc_score(test_sentiments, y_pred)\n",
    "print(f'AUC-ROC: {auc:.4f}')\n",
    "\n",
    "gmean = geometric_mean_score(test_sentiments, y_pred)\n",
    "print(f'G-Mean: {gmean:.4f}')\n",
    "\n",
    "cm = confusion_matrix(test_sentiments, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(cm, ['Class 0', 'Class 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Weighted RVFL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data scaling\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False because sparse matrices do not support mean centering\n",
    "x_train_scaled = scaler.fit_transform(cv_train_reviews)\n",
    "x_val_scaled = scaler.transform(cv_test_reviews)\n",
    "\n",
    "# Define a function to calculate G-Mean\n",
    "def g_mean(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    return np.sqrt(sensitivity * specificity)\n",
    "\n",
    "# Weighted RVFL model (initial setup with RandomForest for comparison)\n",
    "weighted_rvfl_model = RandomForestClassifier(class_weight=\"balanced\", n_estimators=100, random_state=42)\n",
    "weighted_rvfl_model.fit(x_train_scaled, train_sentiments.ravel())\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = weighted_rvfl_model.predict(x_val_scaled)\n",
    "y_prob = weighted_rvfl_model.predict_proba(x_val_scaled)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "auc = roc_auc_score(test_sentiments, y_prob)\n",
    "f1 = f1_score(test_sentiments, y_pred)\n",
    "accuracy = accuracy_score(test_sentiments, y_pred)\n",
    "gmean = g_mean(test_sentiments, y_pred)\n",
    "\n",
    "print(\"Weighted RVFL Performance Metrics:\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"G-Mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Boosting Weighted RVFL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False for sparse matrices\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# RVFL class with Ridge regression\n",
    "class RVFL:\n",
    "    def __init__(self, input_size, hidden_nodes=100, activation_function='relu', ridge_lambda=1.0):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.activation_function = activation_function\n",
    "        self.ridge_lambda = ridge_lambda\n",
    "        self.random_weights = np.random.randn(input_size, hidden_nodes)\n",
    "        self.random_biases = np.random.randn(hidden_nodes)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def leaky_relu(self, x):\n",
    "        return np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "    def transform(self, X):\n",
    "        H = np.dot(X, self.random_weights) + self.random_biases\n",
    "        if self.activation_function == 'relu':\n",
    "            H = self.relu(H)\n",
    "        elif self.activation_function == 'leaky_relu':\n",
    "            H = self.leaky_relu(H)\n",
    "        return np.hstack((X.toarray(), H))  # Convert sparse to dense before stacking\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        H_train = self.transform(X_train)\n",
    "        self.scaler = StandardScaler().fit(H_train)\n",
    "        H_train = self.scaler.transform(H_train)\n",
    "        self.ridge_reg = Ridge(alpha=self.ridge_lambda)\n",
    "        self.ridge_reg.fit(H_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        H_test = self.transform(X_test)\n",
    "        H_test = self.scaler.transform(H_test)\n",
    "        return self.ridge_reg.predict(H_test)\n",
    "\n",
    "    def predict_classes(self, X_test):\n",
    "        return np.where(self.predict(X_test) > 0.5, 1, 0)\n",
    "\n",
    "# Boosting Weighted RVFL with learning rate\n",
    "class BoostingWeightedRVFL:\n",
    "    def __init__(self, num_models=10, hidden_nodes=100, learning_rate=0.1):\n",
    "        self.num_models = num_models\n",
    "        self.models = []\n",
    "        self.model_weights = []\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        N = X_train.shape[0]\n",
    "        weights = np.ones(N) / N\n",
    "\n",
    "        for i in range(self.num_models):\n",
    "            model = RVFL(input_size=X_train.shape[1], hidden_nodes=self.hidden_nodes)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict_classes(X_train)\n",
    "\n",
    "            errors = np.abs(y_train - y_pred)\n",
    "            weighted_error = np.dot(weights, errors) / np.sum(weights)\n",
    "\n",
    "            if weighted_error < 1e-10:\n",
    "                break\n",
    "\n",
    "            alpha = self.learning_rate * np.log((1 - weighted_error) / (weighted_error + 1e-10))\n",
    "            self.models.append(model)\n",
    "            self.model_weights.append(alpha)\n",
    "\n",
    "            weights = weights * np.exp(alpha * errors)\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        final_predictions = np.zeros(X_test.shape[0])\n",
    "        for model, alpha in zip(self.models, self.model_weights):\n",
    "            final_predictions += alpha * model.predict_classes(X_test)\n",
    "        return np.where(final_predictions > 0, 1, 0)\n",
    "\n",
    "# Manual G-Mean calculation\n",
    "def calculate_gmean(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    gmean = np.sqrt(sensitivity * specificity)\n",
    "    return gmean\n",
    "\n",
    "# Training and evaluation\n",
    "model = BoostingWeightedRVFL(num_models=5, hidden_nodes=50, learning_rate=0.001)\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "gmean = calculate_gmean(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"AUC-ROC Score:\", auc_roc)\n",
    "print(\"G-Mean:\", gmean)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Inception V3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define G-Mean function for evaluation\n",
    "def g_mean(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    return np.sqrt(sensitivity * specificity)\n",
    "\n",
    "# Ensure processed data is already in the notebook environment:\n",
    "# tv_train_reviews, tv_test_reviews, train_sentiments, test_sentiments\n",
    "\n",
    "# Convert sparse matrices to dense arrays if needed\n",
    "tv_train_reviews = tv_train_reviews.toarray()\n",
    "tv_test_reviews = tv_test_reviews.toarray()\n",
    "\n",
    "# Model architecture for text data\n",
    "input_layer = Input(shape=(tv_train_reviews.shape[1],))\n",
    "\n",
    "# Multi-layer dense network\n",
    "tower_1 = Dense(128, activation=\"relu\")(input_layer)\n",
    "tower_1 = Dropout(0.3)(tower_1)\n",
    "\n",
    "tower_2 = Dense(64, activation=\"relu\")(input_layer)\n",
    "tower_2 = Dropout(0.3)(tower_2)\n",
    "\n",
    "tower_3 = Dense(32, activation=\"relu\")(input_layer)\n",
    "tower_3 = Dropout(0.3)(tower_3)\n",
    "\n",
    "# Concatenate the layers\n",
    "concatenated = Concatenate()([tower_1, tower_2, tower_3])\n",
    "\n",
    "# Output layer for binary classification\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(concatenated)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    tv_train_reviews, train_sentiments,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,  # Use part of training data for validation\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_probs = model.predict(tv_test_reviews).flatten()  # Sigmoid outputs\n",
    "y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_probs]\n",
    "\n",
    "# Evaluation metrics\n",
    "auc = roc_auc_score(test_sentiments, y_pred_probs)\n",
    "f1 = f1_score(test_sentiments, y_pred)\n",
    "accuracy = accuracy_score(test_sentiments, y_pred)\n",
    "gmean = g_mean(test_sentiments, y_pred)\n",
    "\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"G-Mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming preprocessed data is available in the notebook from prior steps\n",
    "# Extract data from previously defined variables\n",
    "x_train = cv_train_reviews  # Replace with actual variable if different\n",
    "x_test = cv_test_reviews    # Replace with actual variable if different\n",
    "y_train = train_sentiments.ravel()  # Flatten y\n",
    "y_test = test_sentiments.ravel()    # Flatten y\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler(with_mean=False)  # Set with_mean=False to handle sparse matrices\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Define a function to calculate G-Mean\n",
    "def g_mean(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    return np.sqrt(sensitivity * specificity)\n",
    "\n",
    "# Initialize and train the SVM model with class weights to address imbalance\n",
    "svm_model = SVC(kernel=\"linear\", class_weight=\"balanced\", probability=True, random_state=42)\n",
    "svm_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = svm_model.predict(x_test_scaled)\n",
    "y_prob = svm_model.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "gmean = g_mean(y_test, y_pred)\n",
    "\n",
    "# Print out the performance metrics\n",
    "print(\"SVM Performance Metrics:\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"G-Mean: {gmean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the AUC-Based RVFL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume x_train, x_test, y_train, y_test have been preloaded in the same notebook\n",
    "\n",
    "# Define RVFL with Iterative AUC optimization\n",
    "class IterativeAUC_RVFL:\n",
    "    def __init__(self, n_hidden_neurons=100, gamma=0.01, max_iters=100, tol=1e-4):\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.gamma = gamma\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol  # Tolerance for convergence\n",
    "\n",
    "    def initialize_weights_and_biases(self, n_features):\n",
    "        # Random weight and bias initialization\n",
    "        self.input_weights = np.random.randn(n_features, self.n_hidden_neurons)\n",
    "        self.biases = np.random.randn(self.n_hidden_neurons)\n",
    "\n",
    "    def hidden_layer_output(self, X):\n",
    "        # Calculate hidden layer output using ReLU activation\n",
    "        H = np.dot(X, self.input_weights) + self.biases\n",
    "        return np.maximum(H, 0)  # ReLU\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.initialize_weights_and_biases(n_features)\n",
    "\n",
    "        # Generate initial hidden layer output matrix H\n",
    "        H = self.hidden_layer_output(X)\n",
    "        \n",
    "        # Initial computation for matrices A and Y\n",
    "        A = np.zeros((self.n_hidden_neurons, self.n_hidden_neurons))\n",
    "        Y = np.zeros((self.n_hidden_neurons, 1))\n",
    "        for i in range(n_samples):\n",
    "            if y[i] == 1:  # Positive class\n",
    "                A += np.outer(H[i], H[i])\n",
    "                Y += H[i].reshape(-1, 1)\n",
    "            else:  # Negative class\n",
    "                A -= np.outer(H[i], H[i])\n",
    "                Y -= H[i].reshape(-1, 1)\n",
    "        \n",
    "        # Initialize beta with zeros\n",
    "        self.beta = np.zeros((self.n_hidden_neurons, 1))\n",
    "        \n",
    "        # Iterative training to update beta\n",
    "        for iteration in range(self.max_iters):\n",
    "            # Compute output weights beta with iterative improvement\n",
    "            if n_samples > self.n_hidden_neurons:\n",
    "                beta_new = inv((n_samples / self.gamma) * A + np.dot(H.T, H)).dot(H.T).dot(Y)\n",
    "            else:\n",
    "                beta_new = H.T.dot(inv(np.dot(H, H.T) + (n_samples / self.gamma) * A)).dot(Y)\n",
    "            \n",
    "            # Check for convergence based on tolerance\n",
    "            if np.linalg.norm(self.beta - beta_new) < self.tol:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "            \n",
    "            # Update beta for the next iteration\n",
    "            self.beta = beta_new\n",
    "        \n",
    "        # Final beta after iteration\n",
    "        self.beta = beta_new\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Calculate hidden layer output and final predictions\n",
    "        H = self.hidden_layer_output(X)\n",
    "        output = np.dot(H, self.beta)\n",
    "        return np.where(output >= 0.5, 1, 0)\n",
    "\n",
    "# Instantiate and Train Iterative AUC-based RVFL\n",
    "model = IterativeAUC_RVFL(n_hidden_neurons=100, gamma=0.01, max_iters=100, tol=1e-4)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = model.predict(x_test)\n",
    "auc_score = roc_auc_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "g_mean = np.sqrt(confusion_matrix(y_test, y_pred).diagonal().prod() / (y_test.sum() * (len(y_test) - y_test.sum())))\n",
    "\n",
    "print(f\"Iterative AUC-RVFL Performance Metrics:\\n\"\n",
    "      f\"AUC-ROC Score: {auc_score:.4f}\\n\"\n",
    "      f\"Accuracy: {accuracy:.4f}\\n\"\n",
    "      f\"F1 Score: {f1:.4f}\\n\"\n",
    "      f\"G-Mean: {g_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
