{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Meddef Model for Defense Medical Images Against Adversarial Attacks",
   "id": "aad7c9a1673147c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:14:06.451849Z",
     "start_time": "2024-07-09T01:13:48.890632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from loader.dataset_loader import DatasetLoader\n",
    "from tabulate import tabulate\n",
    "from loader.preprocess import Preprocessor\n",
    "from model.base_model import ModelLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the logging level\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ],
   "id": "2100e688ed1c1b0a",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:14:10.204367Z",
     "start_time": "2024-07-09T01:14:06.456416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the data directory\n",
    "data_dir = './dataset'  # Replace with your dataset directory\n",
    "\n",
    "# Get all datasets\n",
    "all_datasets = DatasetLoader.get_all_datasets(data_dir)\n",
    "\n",
    "# Initialize a dictionary to hold the datasets\n",
    "datasets = {}\n",
    "\n",
    "# Initialize classes to an empty list\n",
    "classes = []\n",
    "\n",
    "# Iterate over all datasets\n",
    "for dataset_name in all_datasets:\n",
    "    # Initialize the DatasetLoader\n",
    "    dataset_loader = DatasetLoader(dataset_name, data_dir)\n",
    "\n",
    "    # Load the dataset\n",
    "    train_dataset, val_dataset, test_dataset = dataset_loader.load()\n",
    "\n",
    "    # Print the classes and their counts\n",
    "    try:\n",
    "        classes = dataset_loader.get_and_print_classes()\n",
    "        class_counts = dataset_loader.print_class_counts()\n",
    "    except ValueError as e:\n",
    "        logging.error(e)\n",
    "        raise\n",
    "\n",
    "    # Store the datasets in the dictionary\n",
    "    datasets[dataset_name] = {\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_dataset\n",
    "    }\n",
    "\n"
   ],
   "id": "e923a35ab5ffc5aa",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:14:10.212140Z",
     "start_time": "2024-07-09T01:14:10.205942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_image(dataset, n_rows=2, n_cols=3):\n",
    "    \"\"\"\n",
    "    Display images from a Dataset along with their corresponding labels.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (torch.utils.data.Dataset): Dataset containing images and labels.\n",
    "    - n_rows (int): Number of rows in the display grid.\n",
    "    - n_cols (int): Number of columns in the display grid.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Display n pictures of the dataset with their labels\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 7),\n",
    "                        subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image, label = dataset[i]\n",
    "        ax.imshow(image.permute(1, 2, 0))  # permute the dimensions to match the expected input of imshow\n",
    "        ax.set_title(label)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "d73f433d73b87cb7",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:14:11.135091Z",
     "start_time": "2024-07-09T01:14:10.214659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Iterate over all datasets\n",
    "for dataset_name in datasets:\n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        dataset = datasets[dataset_name][split_name]\n",
    "        print(f\"{split_name.capitalize()} set shape: ({len(dataset)}, 2)\")  # Assuming each dataset item is an image-label pair\n",
    "\n",
    "    # Display the first few images of the training set only\n",
    "    print(\"\\nDisplaying images from the training set:\")\n",
    "    display_image(datasets[dataset_name]['train'])"
   ],
   "id": "69d701a099ebcaa3",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:14:11.251713Z",
     "start_time": "2024-07-09T01:14:11.136096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_head(dataset, num_items=5):\n",
    "    \"\"\"\n",
    "    Display the first few items from a PyTorch Dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (torch.utils.data.Dataset): Dataset containing images and labels.\n",
    "    - num_items (int): Number of items to display.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for i in range(num_items):\n",
    "        image, label = dataset[i]\n",
    "        print(f\"Item {i+1}\")\n",
    "        print(f\"Label: {label}\")\n",
    "        print(f\"Image shape: {image.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Iterate over all datasets\n",
    "for dataset_name in datasets:\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        dataset = datasets[dataset_name][split_name]\n",
    "        print(f\"{split_name.capitalize()} set:\")\n",
    "        display_head(dataset)"
   ],
   "id": "bf70e1b211e22a48",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:14:16.472142Z",
     "start_time": "2024-07-09T01:14:11.252721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_class_counts(all_datasets, dataset_loaders):\n",
    "    # Iterate over all datasets\n",
    "    for dataset_name in all_datasets:\n",
    "        # Create a list to hold the table data\n",
    "        table_data = []\n",
    "\n",
    "        # Get the classes for the current dataset\n",
    "        classes = dataset_loaders[dataset_name].get_and_print_classes()\n",
    "\n",
    "        # Get counts for each dataset\n",
    "        train_counts = Counter(data[1] for data in all_datasets[dataset_name]['train'])\n",
    "        val_counts = Counter(data[1] for data in all_datasets[dataset_name]['val'])\n",
    "        test_counts = Counter(data[1] for data in all_datasets[dataset_name]['test'])\n",
    "\n",
    "        # Get unique labels\n",
    "        unique_labels = sorted(set(train_counts) | set(val_counts) | set(test_counts))\n",
    "\n",
    "        # Iterate through the labels to print and append counts for each dataset\n",
    "        for label in unique_labels:\n",
    "            class_name = label  # Use the label as the class name\n",
    "            train_count = train_counts[label]\n",
    "            val_count = val_counts[label]\n",
    "            test_count = test_counts[label]\n",
    "            table_data.append([class_name, train_count, val_count, test_count])\n",
    "\n",
    "        # Print the table\n",
    "        headers = [\"Class\", \"Training Set\", \"Validation Set\", \"Test Set\"]\n",
    "        print(f\"\\nDataset: {dataset_name}\")\n",
    "        print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "        # Clear the table data\n",
    "        table_data.clear()\n",
    "\n",
    "# Create a dictionary to hold the DatasetLoader instances\n",
    "dataset_loaders = {dataset_name: DatasetLoader(dataset_name, data_dir) for dataset_name in all_datasets}\n",
    "\n",
    "# Print counts for each dataset\n",
    "print_class_counts(datasets, dataset_loaders)"
   ],
   "id": "56201094a0b8e653",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:14:16.475655Z",
     "start_time": "2024-07-09T01:14:16.472142Z"
    }
   },
   "cell_type": "code",
   "source": "# Test",
   "id": "b08eaa72e5854b4f",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:14:16.482473Z",
     "start_time": "2024-07-09T01:14:16.476552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from collections import Counter\n",
    "# \n",
    "# def print_class_counts(all_datasets, dataset_loaders):\n",
    "#     # Iterate over all datasets\n",
    "#     for dataset_name in all_datasets:\n",
    "#         # Create a list to hold the table data\n",
    "#         table_data = []\n",
    "# \n",
    "#         # Get the classes for the current dataset\n",
    "#         classes = dataset_loaders[dataset_name].get_and_print_classes()\n",
    "# \n",
    "#         # Get counts for each dataset\n",
    "#         train_counts = Counter(data[1] for data in all_datasets[dataset_name]['train'])\n",
    "#         val_counts = Counter(data[1] for data in all_datasets[dataset_name]['val'])\n",
    "#         test_counts = Counter(data[1] for data in all_datasets[dataset_name]['test'])\n",
    "# \n",
    "#         # Get unique labels\n",
    "#         unique_labels = sorted(set(train_counts) | set(val_counts) | set(test_counts))\n",
    "# \n",
    "#         # Iterate through the labels to print and append counts for each dataset\n",
    "#         for label in unique_labels:\n",
    "#             class_name = classes[label]  # Use the class name corresponding to the label\n",
    "#             train_count = train_counts[label]\n",
    "#             val_count = val_counts[label]\n",
    "#             test_count = test_counts[label]\n",
    "#             table_data.append([class_name, train_count, val_count, test_count])\n",
    "# \n",
    "#         # Print the table\n",
    "#         headers = [\"Class\", \"Training Set\", \"Validation Set\", \"Test Set\"]\n",
    "#         print(f\"\\nDataset: {dataset_name}\")\n",
    "#         print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "# \n",
    "#         # Clear the table data\n",
    "#         table_data.clear()\n",
    "# \n",
    "# # Create a dictionary to hold the DatasetLoader instances\n",
    "# dataset_loaders = {dataset_name: DatasetLoader(dataset_name, data_dir) for dataset_name in all_datasets}\n",
    "# \n",
    "# # Print counts for each dataset\n",
    "# print_class_counts(datasets, dataset_loaders)"
   ],
   "id": "ab360fa98c522a76",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:14:16.486655Z",
     "start_time": "2024-07-09T01:14:16.483483Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a36dfe985e844c96",
   "execution_count": 9,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
